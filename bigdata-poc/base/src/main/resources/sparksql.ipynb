{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$exclude.$                        , $ivy.$                            // for cleaner logs\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$profile.$           \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   // adjust spark version - spark >= 2.0\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                // for JupyterSparkSession (SparkSession aware of the jupyter-scala kernel)\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                 \n",
       "\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjupyter.spark.session._\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $exclude.`org.slf4j:slf4j-log4j12`, $ivy.`org.slf4j:slf4j-nop:1.7.21` // for cleaner logs\n",
    "import $profile.`hadoop-2.7`\n",
    "import $ivy.`org.apache.spark::spark-sql:2.3.0` // adjust spark version - spark >= 2.0\n",
    "import $ivy.`org.jupyter-scala::spark:0.4.2` // for JupyterSparkSession (SparkSession aware of the jupyter-scala kernel)\n",
    "import $ivy.`org.postgresql:postgresql:42.2.2`\n",
    "\n",
    "\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.sql._\n",
    "import jupyter.spark.session._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SparkSession\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log4j:WARN No appenders could be found for logger (io.netty.util.internal.logging.InternalLoggerFactory).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@79b008cd"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession\n",
    "      .builder\n",
    "      .master(\"local\")\n",
    "      .appName(\"Test\")\n",
    "      .config(\"spark.jars\", \"/mnt/data/dev/workspaces/poc/bigdata-poc/lib/postgresql-42.2.2.jar\")\n",
    "      .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36msc\u001b[39m: \u001b[32mSparkContext\u001b[39m = org.apache.spark.SparkContext@b51951f"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mjdbcHostname\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"localhost\"\u001b[39m\n",
       "\u001b[36mjdbcPort\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m5432\u001b[39m\n",
       "\u001b[36mjdbcDatabase\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"algo\"\u001b[39m\n",
       "\u001b[36mjdbcUsername\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"algo\"\u001b[39m\n",
       "\u001b[36mjdbcPassword\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"algo\"\u001b[39m\n",
       "\u001b[36mdriverClass\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"org.postgresql.Driver\"\u001b[39m\n",
       "\u001b[36mjdbcUrl\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"jdbc:postgresql://localhost:5432/algo\"\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.util.Properties\n",
       "\u001b[39m\n",
       "\u001b[36mconnectionProperties\u001b[39m: \u001b[32mjava\u001b[39m.\u001b[32mutil\u001b[39m.\u001b[32mProperties\u001b[39m = {user=algo, password=algo, Driver=org.postgresql.Driver}\n",
       "\u001b[36mres4_9\u001b[39m: \u001b[32mObject\u001b[39m = null\n",
       "\u001b[36mres4_10\u001b[39m: \u001b[32mObject\u001b[39m = null\n",
       "\u001b[36mres4_11\u001b[39m: \u001b[32mObject\u001b[39m = null"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val jdbcHostname = \"localhost\"\n",
    "val jdbcPort = 5432\n",
    "val jdbcDatabase =\"algo\"\n",
    "val jdbcUsername =\"algo\"\n",
    "val jdbcPassword =\"algo\"\n",
    "val driverClass = \"org.postgresql.Driver\"\n",
    "val jdbcUrl = s\"jdbc:postgresql://${jdbcHostname}:${jdbcPort}/${jdbcDatabase}\"\n",
    "\n",
    "    // Create a Properties() object to hold the parameters.\n",
    "import java.util.Properties\n",
    "val connectionProperties = new Properties()\n",
    "connectionProperties.put(\"user\", s\"${jdbcUsername}\")\n",
    "connectionProperties.put(\"password\", s\"${jdbcPassword}\")\n",
    "connectionProperties.put(\"Driver\", s\"${driverClass}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36macctDF\u001b[39m: \u001b[32mDataFrame\u001b[39m = [acct_id: string, acct_name: string ... 3 more fields]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val acctDF = spark.read.jdbc(jdbcUrl, \"accounts\", connectionProperties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36minstDF\u001b[39m: \u001b[32mDataFrame\u001b[39m = [inst_id: string, symbol: string ... 18 more fields]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val instDF = spark.read.jdbc(jdbcUrl, \"instruments\", connectionProperties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36minstRelDF\u001b[39m: \u001b[32mDataFrame\u001b[39m = [inst_id: string, const_inst_id: string ... 1 more field]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val instRelDF = spark.read.jdbc(jdbcUrl, \"instrument_relations\", connectionProperties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mposDF\u001b[39m: \u001b[32mDataFrame\u001b[39m = [acct_id: string, inst_id: string ... 4 more fields]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val posDF = spark.read.jdbc(jdbcUrl, \"positions\", connectionProperties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions.{expr, col, column}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.Row\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types.{StructField, StructType, StringType, LongType}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types.Metadata\u001b[39m"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import org.apache.spark.sql.functions.{expr, col, column}\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.types.{StructField, StructType, StringType, LongType}\n",
    "import org.apache.spark.sql.types.Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------------+\n",
      "|         avg(weight)|count(DISTINCT inst_id)|\n",
      "+--------------------+-----------------------+\n",
      "|0.002837675453331...|                     14|\n",
      "+--------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "instRelDF.selectExpr(\"avg(weight)\", \"count(distinct(inst_id))\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+--------+\n",
      "|inst_id|const_inst_id|  weight|\n",
      "+-------+-------------+--------+\n",
      "|    QQQ|         AAPL|0.118184|\n",
      "|    QQQ|         AMZN| 0.10031|\n",
      "+-------+-------------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "instRelDF.where(\"weight > 0.10\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "script",
   "pygments_lexer": "scala",
   "version": "2.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
