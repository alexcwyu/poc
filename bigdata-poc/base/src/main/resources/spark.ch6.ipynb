{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log4j:WARN No appenders could be found for logger (io.netty.util.internal.logging.InternalLoggerFactory).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$exclude.$                        , $ivy.$                            // for cleaner logs\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$profile.$           \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   // adjust spark version - spark >= 2.0\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                // for JupyterSparkSession (SparkSession aware of the jupyter-scala kernel)\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                 \n",
       "\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjupyter.spark.session._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SparkSession\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@14ee7615\n",
       "\u001b[36msc\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mSparkContext\u001b[39m = org.apache.spark.SparkContext@4a81b901"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $exclude.`org.slf4j:slf4j-log4j12`, $ivy.`org.slf4j:slf4j-nop:1.7.21` // for cleaner logs\n",
    "import $profile.`hadoop-2.7`\n",
    "import $ivy.`org.apache.spark::spark-sql:2.3.0` // adjust spark version - spark >= 2.0\n",
    "import $ivy.`org.jupyter-scala::spark:0.4.2` // for JupyterSparkSession (SparkSession aware of the jupyter-scala kernel)\n",
    "import $ivy.`org.postgresql:postgresql:42.2.2`\n",
    "\n",
    "\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.sql._\n",
    "import jupyter.spark.session._\n",
    "\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession\n",
    "      .builder\n",
    "      .master(\"local\")\n",
    "      .appName(\"Test\")\n",
    "      .config(\"spark.jars\", \"/mnt/data/dev/workspaces/poc/bigdata-poc/lib/postgresql-42.2.2.jar\")\n",
    "      .getOrCreate()\n",
    "\n",
    "val sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types.{StructField, StructType, StringType, LongType, Metadata}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions.{col, column, expr, lit, get_json_object, json_tuple, udf}\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types.{StructField, StructType, StringType, LongType, Metadata}\n",
    "import org.apache.spark.sql.functions.{col, column, expr, lit, get_json_object, json_tuple, udf}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mjsonDF\u001b[39m: \u001b[32mDataFrame\u001b[39m = [jsonString: string]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val jsonDF = spark.range(1).selectExpr(\"\"\"  '{\"myJSONKey\" : {\"myJSONValue\" : [1, 2, 3]}}' as jsonString\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|column|                  c0|\n",
      "+------+--------------------+\n",
      "|     2|{\"myJSONValue\":[1...|\n",
      "+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsonDF.select(\n",
    "    get_json_object(col(\"jsonString\"), \"$.myJSONKey.myJSONValue[1]\") as \"column\",\n",
    "    json_tuple(col(\"jsonString\"), \"myJSONKey\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mpower3\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def power3(number: Double) : Double = number * number * number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres5\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m64.0\u001b[39m"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "power3(4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mudfExampleDF\u001b[39m: \u001b[32mDataFrame\u001b[39m = [num: bigint]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val udfExampleDF = spark.range(5).toDF(\"num\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres7\u001b[39m: \u001b[32mDataFrame\u001b[39m = [num: bigint]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "udfExampleDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mpower3udf\u001b[39m: \u001b[32mexpressions\u001b[39m.\u001b[32mUserDefinedFunction\u001b[39m = \u001b[33mUserDefinedFunction\u001b[39m(<function1>, DoubleType, \u001b[33mSome\u001b[39m(\u001b[33mList\u001b[39m(DoubleType)))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//DataFrame function:\n",
    "val power3udf = udf(power3(_:Double):Double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|UDF(num)|\n",
      "+--------+\n",
      "|     0.0|\n",
      "|     1.0|\n",
      "|     8.0|\n",
      "|    27.0|\n",
      "|    64.0|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "udfExampleDF.select(power3udf(col(\"num\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|UDF(num)|\n",
      "+--------+\n",
      "|     0.0|\n",
      "|     1.0|\n",
      "+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "udfExampleDF.select(power3udf(col(\"num\"))).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres23\u001b[39m: \u001b[32mexpressions\u001b[39m.\u001b[32mUserDefinedFunction\u001b[39m = \u001b[33mUserDefinedFunction\u001b[39m(<function1>, DoubleType, \u001b[33mSome\u001b[39m(\u001b[33mList\u001b[39m(DoubleType)))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Spark SQL function\n",
    "spark.udf.register(\"power3\", power3(_:Double):Double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31morg.apache.spark.SparkException: Task not serializable\u001b[39m",
      "  org.apache.spark.util.ClosureCleaner$.ensureSerializable(\u001b[32mClosureCleaner.scala\u001b[39m:\u001b[32m345\u001b[39m)",
      "  org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(\u001b[32mClosureCleaner.scala\u001b[39m:\u001b[32m335\u001b[39m)",
      "  org.apache.spark.util.ClosureCleaner$.clean(\u001b[32mClosureCleaner.scala\u001b[39m:\u001b[32m159\u001b[39m)",
      "  org.apache.spark.SparkContext.clean(\u001b[32mSparkContext.scala\u001b[39m:\u001b[32m2292\u001b[39m)",
      "  org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1.apply(\u001b[32mRDD.scala\u001b[39m:\u001b[32m844\u001b[39m)",
      "  org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1.apply(\u001b[32mRDD.scala\u001b[39m:\u001b[32m843\u001b[39m)",
      "  org.apache.spark.rdd.RDDOperationScope$.withScope(\u001b[32mRDDOperationScope.scala\u001b[39m:\u001b[32m151\u001b[39m)",
      "  org.apache.spark.rdd.RDDOperationScope$.withScope(\u001b[32mRDDOperationScope.scala\u001b[39m:\u001b[32m112\u001b[39m)",
      "  org.apache.spark.rdd.RDD.withScope(\u001b[32mRDD.scala\u001b[39m:\u001b[32m363\u001b[39m)",
      "  org.apache.spark.rdd.RDD.mapPartitionsWithIndex(\u001b[32mRDD.scala\u001b[39m:\u001b[32m843\u001b[39m)",
      "  org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(\u001b[32mWholeStageCodegenExec.scala\u001b[39m:\u001b[32m608\u001b[39m)",
      "  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m131\u001b[39m)",
      "  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)",
      "  org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m155\u001b[39m)",
      "  org.apache.spark.rdd.RDDOperationScope$.withScope(\u001b[32mRDDOperationScope.scala\u001b[39m:\u001b[32m151\u001b[39m)",
      "  org.apache.spark.sql.execution.SparkPlan.executeQuery(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m152\u001b[39m)",
      "  org.apache.spark.sql.execution.SparkPlan.execute(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)",
      "  org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m247\u001b[39m)",
      "  org.apache.spark.sql.execution.SparkPlan.executeTake(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m337\u001b[39m)",
      "  org.apache.spark.sql.execution.CollectLimitExec.executeCollect(\u001b[32mlimit.scala\u001b[39m:\u001b[32m38\u001b[39m)",
      "  org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(\u001b[32mDataset.scala\u001b[39m:\u001b[32m3272\u001b[39m)",
      "  org.apache.spark.sql.Dataset$$anonfun$head$1.apply(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2484\u001b[39m)",
      "  org.apache.spark.sql.Dataset$$anonfun$head$1.apply(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2484\u001b[39m)",
      "  org.apache.spark.sql.Dataset$$anonfun$52.apply(\u001b[32mDataset.scala\u001b[39m:\u001b[32m3253\u001b[39m)",
      "  org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m77\u001b[39m)",
      "  org.apache.spark.sql.Dataset.withAction(\u001b[32mDataset.scala\u001b[39m:\u001b[32m3252\u001b[39m)",
      "  org.apache.spark.sql.Dataset.head(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2484\u001b[39m)",
      "  org.apache.spark.sql.Dataset.take(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2698\u001b[39m)",
      "  org.apache.spark.sql.Dataset.showString(\u001b[32mDataset.scala\u001b[39m:\u001b[32m254\u001b[39m)",
      "  org.apache.spark.sql.Dataset.show(\u001b[32mDataset.scala\u001b[39m:\u001b[32m723\u001b[39m)",
      "  org.apache.spark.sql.Dataset.show(\u001b[32mDataset.scala\u001b[39m:\u001b[32m682\u001b[39m)",
      "  $sess.cmd25Wrapper$Helper.<init>(\u001b[32mcmd25.sc\u001b[39m:\u001b[32m2\u001b[39m)",
      "  $sess.cmd25Wrapper.<init>(\u001b[32mcmd25.sc\u001b[39m:\u001b[32m373\u001b[39m)",
      "  $sess.cmd25$.<init>(\u001b[32mcmd25.sc\u001b[39m:\u001b[32m278\u001b[39m)",
      "  $sess.cmd25$.<clinit>(\u001b[32mcmd25.sc\u001b[39m:\u001b[32m-1\u001b[39m)",
      "\u001b[31mjava.io.NotSerializableException: org.apache.spark.SparkContext",
      "Serialization stack:",
      "\t- object not serializable (class: org.apache.spark.SparkContext, value: org.apache.spark.SparkContext@4a81b901)",
      "\t- field (class: $sess.cmd0Wrapper$Helper, name: sc, type: class org.apache.spark.SparkContext)",
      "\t- object (class $sess.cmd0Wrapper$Helper, $sess.cmd0Wrapper$Helper@4f38816)",
      "\t- field (class: $sess.cmd0Wrapper, name: wrapper, type: class $sess.cmd0Wrapper$Helper)",
      "\t- object (class $sess.cmd0Wrapper, $sess.cmd0Wrapper@7309a788)",
      "\t- field (class: $sess.cmd25Wrapper, name: cmd0, type: class $sess.cmd0Wrapper)",
      "\t- object (class $sess.cmd25Wrapper, $sess.cmd25Wrapper@11fdd376)",
      "\t- field (class: $sess.cmd25Wrapper$Helper, name: $outer, type: class $sess.cmd25Wrapper)",
      "\t- object (class $sess.cmd25Wrapper$Helper, $sess.cmd25Wrapper$Helper@41dccdcd)",
      "\t- field (class: $sess.cmd25Wrapper$Helper$$anonfun$1, name: $outer, type: class $sess.cmd25Wrapper$Helper)",
      "\t- object (class $sess.cmd25Wrapper$Helper$$anonfun$1, <function1>)",
      "\t- element of array (index: 3)",
      "\t- array (class [Ljava.lang.Object;, size 4)",
      "\t- field (class: org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10, name: references$1, type: class [Ljava.lang.Object;)",
      "\t- object (class org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10, <function2>)\u001b[39m",
      "  org.apache.spark.serializer.SerializationDebugger$.improveException(\u001b[32mSerializationDebugger.scala\u001b[39m:\u001b[32m40\u001b[39m)",
      "  org.apache.spark.serializer.JavaSerializationStream.writeObject(\u001b[32mJavaSerializer.scala\u001b[39m:\u001b[32m46\u001b[39m)",
      "  org.apache.spark.serializer.JavaSerializerInstance.serialize(\u001b[32mJavaSerializer.scala\u001b[39m:\u001b[32m100\u001b[39m)",
      "  org.apache.spark.util.ClosureCleaner$.ensureSerializable(\u001b[32mClosureCleaner.scala\u001b[39m:\u001b[32m342\u001b[39m)",
      "  org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(\u001b[32mClosureCleaner.scala\u001b[39m:\u001b[32m335\u001b[39m)",
      "  org.apache.spark.util.ClosureCleaner$.clean(\u001b[32mClosureCleaner.scala\u001b[39m:\u001b[32m159\u001b[39m)",
      "  org.apache.spark.SparkContext.clean(\u001b[32mSparkContext.scala\u001b[39m:\u001b[32m2292\u001b[39m)",
      "  org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1.apply(\u001b[32mRDD.scala\u001b[39m:\u001b[32m844\u001b[39m)",
      "  org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1.apply(\u001b[32mRDD.scala\u001b[39m:\u001b[32m843\u001b[39m)",
      "  org.apache.spark.rdd.RDDOperationScope$.withScope(\u001b[32mRDDOperationScope.scala\u001b[39m:\u001b[32m151\u001b[39m)",
      "  org.apache.spark.rdd.RDDOperationScope$.withScope(\u001b[32mRDDOperationScope.scala\u001b[39m:\u001b[32m112\u001b[39m)",
      "  org.apache.spark.rdd.RDD.withScope(\u001b[32mRDD.scala\u001b[39m:\u001b[32m363\u001b[39m)",
      "  org.apache.spark.rdd.RDD.mapPartitionsWithIndex(\u001b[32mRDD.scala\u001b[39m:\u001b[32m843\u001b[39m)",
      "  org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(\u001b[32mWholeStageCodegenExec.scala\u001b[39m:\u001b[32m608\u001b[39m)",
      "  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m131\u001b[39m)",
      "  org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)",
      "  org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m155\u001b[39m)",
      "  org.apache.spark.rdd.RDDOperationScope$.withScope(\u001b[32mRDDOperationScope.scala\u001b[39m:\u001b[32m151\u001b[39m)",
      "  org.apache.spark.sql.execution.SparkPlan.executeQuery(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m152\u001b[39m)",
      "  org.apache.spark.sql.execution.SparkPlan.execute(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)",
      "  org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m247\u001b[39m)",
      "  org.apache.spark.sql.execution.SparkPlan.executeTake(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m337\u001b[39m)",
      "  org.apache.spark.sql.execution.CollectLimitExec.executeCollect(\u001b[32mlimit.scala\u001b[39m:\u001b[32m38\u001b[39m)",
      "  org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(\u001b[32mDataset.scala\u001b[39m:\u001b[32m3272\u001b[39m)",
      "  org.apache.spark.sql.Dataset$$anonfun$head$1.apply(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2484\u001b[39m)",
      "  org.apache.spark.sql.Dataset$$anonfun$head$1.apply(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2484\u001b[39m)",
      "  org.apache.spark.sql.Dataset$$anonfun$52.apply(\u001b[32mDataset.scala\u001b[39m:\u001b[32m3253\u001b[39m)",
      "  org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m77\u001b[39m)",
      "  org.apache.spark.sql.Dataset.withAction(\u001b[32mDataset.scala\u001b[39m:\u001b[32m3252\u001b[39m)",
      "  org.apache.spark.sql.Dataset.head(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2484\u001b[39m)",
      "  org.apache.spark.sql.Dataset.take(\u001b[32mDataset.scala\u001b[39m:\u001b[32m2698\u001b[39m)",
      "  org.apache.spark.sql.Dataset.showString(\u001b[32mDataset.scala\u001b[39m:\u001b[32m254\u001b[39m)",
      "  org.apache.spark.sql.Dataset.show(\u001b[32mDataset.scala\u001b[39m:\u001b[32m723\u001b[39m)",
      "  org.apache.spark.sql.Dataset.show(\u001b[32mDataset.scala\u001b[39m:\u001b[32m682\u001b[39m)",
      "  $sess.cmd25Wrapper$Helper.<init>(\u001b[32mcmd25.sc\u001b[39m:\u001b[32m2\u001b[39m)",
      "  $sess.cmd25Wrapper.<init>(\u001b[32mcmd25.sc\u001b[39m:\u001b[32m373\u001b[39m)",
      "  $sess.cmd25$.<init>(\u001b[32mcmd25.sc\u001b[39m:\u001b[32m278\u001b[39m)",
      "  $sess.cmd25$.<clinit>(\u001b[32mcmd25.sc\u001b[39m:\u001b[32m-1\u001b[39m)"
     ]
    }
   ],
   "source": [
    "spark.udf.register(\"power3\", power3(_:Double):Double)\n",
    "udfExampleDF.selectExpr(\"power3(num)\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "script",
   "pygments_lexer": "scala",
   "version": "2.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
